{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import mmap\n",
    "import random\n",
    "from torch.nn import functional as F\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser(description=\"This is a demonstration program\")\n",
    "# parser.add_argument(\"-batch_size\",type=int,required=True,help=\"Please provide a batch_size\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 128\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "max_iters = 10\n",
    "eval_iters = 2\n",
    "dropout = 0.2\n",
    "n_head = 8\n",
    "n_embd = 384\n",
    "n_layer = 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\x00', '\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x80', '\\x81', '\\x82', '\\x83', '\\x84', '\\x85', '\\x86', '\\x87', '\\x88', '\\x89', '\\x8a', '\\x8b', '\\x8c', '\\x8d', '\\x8e', '\\x8f', '\\x90', '\\x91', '\\x92', '\\x93', '\\x94', '\\x95', '\\x96', '\\x97', '\\x98', '\\x99', '\\x9a', '\\x9b', '\\x9c', '\\x9d', '\\x9e', '\\x9f', '\\xa0', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '\\xad', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Í', 'Î', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Ü', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'ą', 'Ć', 'ć', 'Č', 'č', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'ĕ', 'ė', 'ę', 'ě', 'ğ', 'Ħ', 'ĩ', 'ī', 'ĭ', 'İ', 'ı', 'ķ', 'ļ', 'Ľ', 'Ł', 'ł', 'ń', 'ņ', 'ň', 'ŋ', 'Ō', 'ō', 'ő', 'œ', 'ř', 'Ś', 'ś', 'Ş', 'ş', 'Š', 'š', 'Ţ', 'ţ', 'ť', 'ũ', 'ū', 'ŭ', 'ů', 'ű', 'ŵ', 'ź', 'ż', 'Ž', 'ž', 'Ɖ', 'ơ', 'ư', 'ǎ', 'ǐ', 'ǒ', 'ǔ', 'ǣ', 'Ș', 'ș', 'ț', 'ȩ', 'ɸ', 'ʰ', 'ʻ', 'ʼ', 'ʿ', '˚', '́', '̇', '̈', '̋', '̱', '̲', 'Ό', 'Α', 'Β', 'Γ', 'Δ', 'Ε', 'Ζ', 'Η', 'Θ', 'Κ', 'Λ', 'Π', 'Σ', 'Τ', 'Φ', 'Χ', 'Ω', 'ά', 'έ', 'ή', 'ί', 'α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'χ', 'ω', 'ό', 'ύ', 'ώ', 'А', 'Б', 'В', 'Д', 'Е', 'И', 'К', 'Л', 'М', 'О', 'П', 'Р', 'Ф', 'Ц', 'Ч', 'Ъ', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'х', 'ц', 'ч', 'ш', 'ъ', 'ь', 'я', 'і', 'ѣ', 'ө', 'Ա', 'Գ', 'Դ', 'Ծ', 'Հ', 'Ջ', 'Ս', 'Վ', 'ա', 'բ', 'գ', 'ե', 'զ', 'ի', 'լ', 'խ', 'կ', 'ղ', 'մ', 'ն', 'ո', 'չ', 'ռ', 'ս', 'տ', 'ր', 'ւ', 'ք', 'ֶ', 'ָ', 'ֹ', 'א', 'ב', 'ג', 'ד', 'ה', 'ו', 'ח', 'י', 'ן', 'ס', 'ף', 'פ', 'ת', '،', 'أ', 'إ', 'ا', 'ب', 'ة', 'ت', 'ج', 'خ', 'د', 'ر', 'س', 'ش', 'ط', 'ظ', 'ع', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'ي', 'क', 'ग', 'त', 'द', 'न', 'य', 'र', 'व', 'ह', 'ु', 'े', '्', 'එ', 'ත', 'න', 'ම', 'ය', 'හ', 'ළ', '්', 'ැ', 'ි', 'ა', 'ბ', 'გ', 'ე', 'ვ', 'ზ', 'თ', 'ი', 'ლ', 'მ', 'ნ', 'ო', 'რ', 'ს', 'უ', 'ქ', 'ყ', 'ძ', 'ხ', 'ჯ', '჻', 'ḍ', 'Ḥ', 'ḥ', 'ṃ', 'ṅ', 'ṇ', 'ṙ', 'ṛ', 'ṣ', 'ṭ', 'ẓ', 'ạ', 'ả', 'ậ', 'ắ', 'ằ', 'ế', 'ệ', 'ị', 'ọ', 'ỏ', 'ồ', 'ộ', 'ờ', 'Ụ', 'ụ', 'ừ', 'ỳ', 'ἀ', 'ἕ', 'ὄ', 'ὲ', 'ὶ', 'ᾧ', 'ῆ', 'ῖ', 'ῷ', '\\u2003', '\\u2004', '\\u2006', '\\u2009', '\\u200a', '‐', '‑', '‒', '–', '—', '―', '‘', '’', '“', '”', '„', '†', '•', '‣', '…', '\\u2028', '′', '″', '\\u2060', '₂', '₣', '₤', '₦', '₩', '€', '₱', '₹', '⅓', '⅔', '⅛', '⅜', '←', '→', '∎', '−', '∧', '∨', '≈', '≡', '≤', '≥', '⊆', '⊗', '▪', '◦', '☆', '♥', '♭', '♯', '⬇', '《', '》', 'お', 'か', 'さ', 'た', 'と', 'や', 'ア', 'イ', 'エ', 'オ', 'カ', 'ガ', 'キ', 'ク', 'ケ', 'サ', 'シ', 'ジ', 'ス', 'ズ', 'タ', 'ダ', 'チ', 'ト', 'ド', 'ナ', 'ノ', 'ブ', 'ボ', 'マ', 'ミ', 'ム', 'メ', 'ョ', 'リ', 'ロ', 'ワ', 'ン', 'ー', '一', '下', '不', '与', '中', '主', '也', '了', '五', '些', '人', '代', '们', '余', '你', '儉', '先', '八', '共', '养', '利', '劉', '办', '南', '卦', '去', '双', '口', '可', '名', '后', '和', '咖', '哈', '啡', '因', '国', '圖', '地', '复', '天', '奥', '子', '孩', '學', '宗', '实', '寂', '寞', '对', '就', '山', '岐', '川', '己', '市', '师', '常', '平', '年', '庆', '座', '建', '恢', '息', '慈', '我', '援', '支', '敢', '新', '方', '是', '曲', '村', '案', '楽', '民', '氣', '汶', '治', '法', '波', '流', '港', '火', '灾', '為', '爱', '特', '玉', '王', '的', '看', '着', '米', '精', '给', '耿', '育', '脚', '腿', '自', '與', '興', '要', '誠', '議', '让', '豹', '資', '质', '资', '走', '车', '进', '通', '造', '進', '道', '那', '郎', '重', '鉴', '鑒', '阜', '隆', '震', '靖', '非', '靠', '音', '龍', '龙', '\\uf061', '\\uf0b7', 'ﬂ', '（', '，', '：', '￥']\n"
     ]
    }
   ],
   "source": [
    "chars = \"\"\n",
    "with open('./vocab.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([661,   2, 737,   2, 686,   2, 423,   2, 135,   2, 305,   2, 209,   2,\n",
      "        599,   2, 191,   2, 538,   2, 270,   2, 653,   2, 422,   2, 330,   2,\n",
      "        150,   2, 252,   2,  24,   2, 286,   2, 425,   2, 523,   2,  94,   2,\n",
      "        221,   2, 275,   2, 446,   2, 310,   2, 628,   2, 485,   2, 329,   2,\n",
      "        115,   2, 541,   2, 268,   2, 272,   2, 116,   2, 444,   2, 321,   2,\n",
      "         88,   2,  55,   2, 233,   2,  79,   2, 693,   2, 254,   2, 533,   2,\n",
      "        687,   2, 194,   2, 752,   2, 654,   2,   6,   2, 154,   2,  50,   2,\n",
      "        581,   2])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s : [string_to_int[c] for c in s]\n",
    "decode = lambda l:\"\".join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text),dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[  2, 401,   2,  ..., 290,   2, 153],\n",
      "        [233,   2,  79,  ...,   2, 596,   2],\n",
      "        [  2, 219,   2,  ..., 476,   2, 573],\n",
      "        ...,\n",
      "        [  2, 438,   2,  ..., 729,   2, 325],\n",
      "        [486,   2, 437,  ...,   2, 133,   2],\n",
      "        [  2, 284,   2,  ..., 365,   2,  96]])\n",
      "targets:\n",
      "tensor([[401,   2, 471,  ...,   2, 153,   2],\n",
      "        [  2,  79,   2,  ..., 596,   2,  17],\n",
      "        [219,   2,  22,  ...,   2, 573,   2],\n",
      "        ...,\n",
      "        [438,   2,  38,  ...,   2, 325,   2],\n",
      "        [  2, 437,   2,  ..., 133,   2, 738],\n",
      "        [284,   2, 711,  ...,   2,  96,   2]])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "def get_random_chunk(split):\n",
    "    filename = \"tarin_split.txt\" if split == \"train\" else \"val_split.txt\"\n",
    "    with mmap.mmap(f.fileno(),0,access=mmap.ACCESS_READ) as mm:\n",
    "        file_size = len(mm)\n",
    "        start_pos = random.randint(0,(file_size) - block_size*batch_size)\n",
    "\n",
    "        mm.seek(start_pos)\n",
    "        block = mm.read(block_size*batch_size - 1)\n",
    "        decode_block = block.decode('utf-8',error=\"ignore\").replace('\\r',\"\")\n",
    "\n",
    "        data = torch.tensor(encode(decode_block),dtype=torch.long)\n",
    "    return data\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size,(batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "x,y = get_batch('train')\n",
    "print('inputs:')\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train','var']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            \n",
    "            X,Y = get_batch(split)\n",
    "            logits,loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.query = nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.value = nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) * x.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]  == 0,float('-inf'))\n",
    "        wei = F.softmax(wei,dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,n_head,head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(head_size * n_head,n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        out = torch.cat([h(x) for h in self.heads],dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd,4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd,n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,n_embd,n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head,head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x \n",
    "\n",
    "class GPTLangugeModel(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size,n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd,n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd,vocab_size)\n",
    "        self.apply(self.__init__weights)\n",
    "\n",
    "    def __init__weights(self,module):\n",
    "        if isinstance(module,nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module,nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
    "\n",
    "    def forward(self,index,target=None):\n",
    "        \n",
    "        B,T = index.shape\n",
    "        tok_emb = self.token_embedding_table(index)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T,device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits,target)\n",
    "\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self,index,max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits,loss = self.forward(index)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            index_next = torch.multinomial(probs,num_samples=1)\n",
    "            index = torch.cat((index,index_next),dim=1)\n",
    "        return index\n",
    "    \n",
    "model = GPTLangugeModel(vocab_size)\n",
    "with open('model-01.pkl',\"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "print(\"load successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0,loss:{'train': tensor(6.6261), 'var': tensor(6.6183)}\n",
      "step:2,loss:{'train': tensor(3.9768), 'var': tensor(4.0035)}\n",
      "step:4,loss:{'train': tensor(3.8864), 'var': tensor(4.0211)}\n",
      "step:6,loss:{'train': tensor(3.7613), 'var': tensor(4.0458)}\n",
      "step:8,loss:{'train': tensor(3.5302), 'var': tensor(3.9087)}\n",
      "3.455481767654419\n",
      "model save\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if (iter % eval_iters) == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'step:{iter},loss:{losses}')\n",
    "    xb,yb = get_batch('train')\n",
    "    \n",
    "    logits,loss = model.forward(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())\n",
    "\n",
    "with open('model-01.pkl','wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "print(\"model save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
